# Aspect-Based Sentiment Analysis: Amazon Electronics Reviews

---

## ğŸ¯ Project Overview
This project implements a comprehensive **Aspect-Based Sentiment Analysis (ABSA)** system for Amazon Electronics reviews.  
It explores how different product aspectsâ€”such as price, battery, and designâ€”affect overall sentiment predictions and investigates whether fine-grained, aspect-level analysis provides deeper insights than traditional document-level sentiment classification.

## ğŸ” Key Research Question
> **How effective is aspect-based sentiment analysis compared to traditional sentiment classification, and which product aspects most strongly influence overall review sentiment?**

## ğŸ† Key Achievements
- **Hybrid Feature Engineering** â€“ Combined TF-IDF vectorization with aspect-level sentiment scores.  
- **Multi-Model Implementation** â€“ Logistic Regression, Naive Bayes, and LSTM with embeddings.  
- **Aspect Importance Analysis** â€“ Identified **price**, **performance**, and **battery** as the most influential aspects.  
- **Robust Evaluation** â€“ 81.2 % accuracy with LSTM, using 5-fold cross-validation.  
- **Negativity Bias Detection** â€“ Quantified a 6.6 pp drop in positive predictions for mixed-sentiment reviews.

---

## ğŸ“Š Dataset
- **Source:** Amazon Product Data â€“ *Electronics* category  
- **Size:** 50 000 reviews  
- **Features:** review text, ratings, verified-purchase flag, timestamps  
- **Target:** binary sentiment (*positive* â‰¥ 4 stars, *negative* < 4 stars)  
- **Class distribution:** 82 % positive, 18 % negative  

---

## ğŸ› ï¸ Technical Architecture

### Aspect Extraction System
```python
product_aspects = {
    "performance": ["speed", "processing", "efficiency", "power", "capability"],
    "battery":      ["battery", "charge", "power consumption", "runtime"],
    "design":       ["build", "design", "size", "weight", "appearance"],
    "durability":   ["quality", "durability", "reliability", "robust"],
    "features":     ["feature", "capability", "function", "specification"],
    "price":        ["cost", "price", "value", "affordable", "expensive"],
    "customer_support": ["support", "service", "help", "assistance"],
    "compatibility":    ["compatibility", "integration", "work with"]
}


## ğŸ› ï¸ Feature-Engineering Pipeline
- **Pre-processing** â€” HTML entity decoding, lower-casing, whitespace normalization  
- **Aspect Detection** â€” spaCy + keyword matching  
- **Sentiment Scoring** â€” NLTK Opinion Lexicon  
- **Feature Fusion** â€” TF-IDF (5 000 dimensions) + 8 aspect-sentiment scores  

---

### Model Implementations
| Model                | Features        | Accuracy | F1-neg | Notes                           |
|----------------------|-----------------|----------|--------|---------------------------------|
| Logistic Regression  | 5 008 (hybrid)  | 80.1 %   | 0.585  | L2 regularization, C = 0.1      |
| Naive Bayes          | Hybrid          | 76.9 %   | 0.537  | Class-weighted                  |
| LSTM                 | 100-d embeddings| **81.2 %** | 0.569  | 200-token sequences, 20 k vocab |

---

## ğŸ“ˆ Results Summary
| Model                | Precision (Neg) | Recall (Neg) | F1 (Neg) | Precision (Pos) | Recall (Pos) | F1 (Pos) |
|----------------------|-----------------|--------------|----------|-----------------|--------------|----------|
| **LSTM**             | 0.484           | 0.690        | 0.569    | 0.925           | 0.839        | 0.880    |
| Logistic Regression  | 0.467           | 0.784        | **0.585**| **0.945**       | 0.805        | 0.869    |
| Naive Bayes          | 0.420           | 0.746        | 0.537    | 0.933           | 0.775        | 0.847    |

---

## ğŸ¯ Aspect Importance Rankings
1. **Price** â€” coefficient = 0.344 (strongest)  
2. **Performance** â€” 0.267  
3. **Battery** â€” 0.216  
4. **Compatibility** â€” 0.191  

---

## ğŸ§  Key Insights

### Negativity Bias
- Mixed-sentiment reviews (negative aspects **+** positive rating) cut positive predictions by **6.6 percentage points**.  
- 95 % CI: **[ 5.9 , 7.4 ]**.  
- **Takeaway:** Negative aspects wield disproportionate influence.

### Model Comparison
- **LSTM** â€” Highest accuracy; best at capturing sequence context.  
- **Logistic Regression** â€” Most interpretable; ideal for explaining aspect weights.  
- **Naive Bayes** â€” Fastest training; solid baseline.

---

## ğŸš€ Quick Start

### Installation
```bash
git clone https://github.com/yourusername/aspect-sentiment-analysis.git
cd aspect-sentiment-analysis
pip install -r requirements.txt


### Data Preparation
python src/data_preprocessing.py \
  --input  data/raw/reviews.json.gz \
  --output data/processed/

### Training
Train all models:
python src/train_models.py --config configs/training_config.yaml

Train a specific model:
python src/train_models.py --model lstm --features hybrid

### Evaluation
python src/evaluate.py \
  --model-path models/lstm_model.h5 \
  --test-data data/processed/test.csv

# ğŸ“ Project Structure

aspect-sentiment-analysis/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ model_config.yaml
â”‚   â””â”€â”€ training_config.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ external/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ trained_models/
â”‚   â””â”€â”€ model_artifacts/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_aspect_extraction.ipynb
â”‚   â”œâ”€â”€ 03_model_training.ipynb
â”‚   â””â”€â”€ 04_results_analysis.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ aspect_extractor.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logistic_model.py
â”‚   â”‚   â”œâ”€â”€ naive_bayes_model.py
â”‚   â”‚   â””â”€â”€ lstm_model.py
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ visualization.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ text_processing.py
â”‚       â””â”€â”€ config.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_evaluation.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ reports/
â”‚   â””â”€â”€ model_performance/
â””â”€â”€ docs/
    â”œâ”€â”€ methodology.md
    â”œâ”€â”€ results_analysis.md
    â””â”€â”€ api_reference.md

# Methodology
##Â Evaluation Strategy
- 5-fold stratified cross-validation
- Class imbalance handled via sample weighting (â‰ˆ 4.6 : 1)
- Metrics: Accuracy Â· Precision Â· Recall Â· F1 for each class
- Intrinsic evaluation of aspect extraction â€” P = 0.94 Â· R = 0.59 Â· F1 = 0.73

## Innovation Highlights
- Hybrid TF-IDF + aspect sentiment features
- Negativity-bias quantification with confidence intervals
- Traditional ML vs. Deep Learning side-by-side comparison

## Research Context
This project advances sentiment analysis by:
- Moving from document-level to aspect-level granularity
- Fusing statistical and semantic features
- Delivering actionable insights for product teams

# Tools used
- NLP: spaCy Â· NLTK Â· TensorFlow/Keras
- ML / Data: Scikit-learn Â· NumPy Â· Pandas
- Visualization: Matplotlib Â· Seaborn
- Dev Tools: Jupyter Â· Git Â· Python 3.8 +

# Citation
@misc{aspect_sentiment_analysis_2024,
  title        = {Aspect-Based Sentiment Analysis for Amazon Electronics Reviews},
  author       = {Umberto Belluzzo},
  year         = {2024},
  howpublished = {\url{https://github.com/umbertobelluzzo/aspect-sentiment-analysis}}
}

# Contributing
Pull requests are welcome! See CONTRIBUTING.md for guidelines.

# Acknowledgements
- Amazon Product Data - courtesy of https://nijianmo.github.io/amazon/
- NLTK Opinion Lexicon â€” sentiment lexicon
- spaCy â€” NLP toolkit


